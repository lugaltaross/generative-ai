{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1252de",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81a37d",
   "metadata": {},
   "source": [
    "https://keras.io/examples/generative/gan_ada/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146919e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778269a3",
   "metadata": {},
   "source": [
    "# Data-efficient GANs with Adaptive Discriminator Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027753f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26e51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e122a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ca16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec43f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "num_epochs = 10  # train for 400 epochs for good results\n",
    "image_size = 64\n",
    "# resolution of Kernel Inception Distance measurement, see related section\n",
    "kid_image_size = 75\n",
    "padding = 0.25\n",
    "dataset_name = \"caltech_birds2011\"\n",
    "\n",
    "# adaptive discriminator augmentation\n",
    "max_translation = 0.125\n",
    "max_rotation = 0.125\n",
    "max_zoom = 0.25\n",
    "target_accuracy = 0.85\n",
    "integration_steps = 1000\n",
    "\n",
    "# architecture\n",
    "noise_size = 64\n",
    "depth = 4\n",
    "width = 128\n",
    "leaky_relu_slope = 0.2\n",
    "dropout_rate = 0.4\n",
    "\n",
    "# optimization\n",
    "batch_size = 128\n",
    "learning_rate = 2e-4\n",
    "beta_1 = 0.5  # not using the default value of 0.9 is important\n",
    "ema = 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f505f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_int(float_value):\n",
    "    return tf.cast(tf.math.round(float_value), dtype=tf.int32)\n",
    "\n",
    "\n",
    "def preprocess_image(data):\n",
    "    # unnormalize bounding box coordinates\n",
    "    height = tf.cast(tf.shape(data[\"image\"])[0], dtype=tf.float32)\n",
    "    width = tf.cast(tf.shape(data[\"image\"])[1], dtype=tf.float32)\n",
    "    bounding_box = data[\"bbox\"] * tf.stack([height, width, height, width])\n",
    "\n",
    "    # calculate center and length of longer side, add padding\n",
    "    target_center_y = 0.5 * (bounding_box[0] + bounding_box[2])\n",
    "    target_center_x = 0.5 * (bounding_box[1] + bounding_box[3])\n",
    "    target_size = tf.maximum(\n",
    "        (1.0 + padding) * (bounding_box[2] - bounding_box[0]),\n",
    "        (1.0 + padding) * (bounding_box[3] - bounding_box[1]),\n",
    "    )\n",
    "\n",
    "    # modify crop size to fit into image\n",
    "    target_height = tf.reduce_min(\n",
    "        [target_size, 2.0 * target_center_y, 2.0 * (height - target_center_y)]\n",
    "    )\n",
    "    target_width = tf.reduce_min(\n",
    "        [target_size, 2.0 * target_center_x, 2.0 * (width - target_center_x)]\n",
    "    )\n",
    "\n",
    "    # crop image\n",
    "    image = tf.image.crop_to_bounding_box(\n",
    "        data[\"image\"],\n",
    "        offset_height=round_to_int(target_center_y - 0.5 * target_height),\n",
    "        offset_width=round_to_int(target_center_x - 0.5 * target_width),\n",
    "        target_height=round_to_int(target_height),\n",
    "        target_width=round_to_int(target_width),\n",
    "    )\n",
    "\n",
    "    # resize and clip\n",
    "    # for image downsampling, area interpolation is the preferred method\n",
    "    image = tf.image.resize(\n",
    "        image, size=[image_size, image_size], method=tf.image.ResizeMethod.AREA\n",
    "    )\n",
    "    return tf.clip_by_value(image / 255.0, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def prepare_dataset(split):\n",
    "    # the validation dataset is shuffled as well, because data order matters\n",
    "    # for the KID calculation\n",
    "    return (\n",
    "        tfds.load(dataset_name, split=split, shuffle_files=True)\n",
    "        .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .shuffle(10 * batch_size)\n",
    "        .batch(batch_size, drop_remainder=True)\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "\n",
    "train_dataset = prepare_dataset(\"train\")\n",
    "val_dataset = prepare_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531f93b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KID(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"kid\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        # KID is estimated per batch and is averaged across batches\n",
    "        self.kid_tracker = keras.metrics.Mean()\n",
    "\n",
    "        # a pretrained InceptionV3 is used without its classification layer\n",
    "        # transform the pixel values to the 0-255 range, then use the same\n",
    "        # preprocessing as during pretraining\n",
    "        self.encoder = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                layers.Rescaling(255.0),\n",
    "                layers.Resizing(height=kid_image_size, width=kid_image_size),\n",
    "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
    "                keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(kid_image_size, kid_image_size, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n",
    "        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(\n",
    "            kernel_generated * (1.0 - tf.eye(batch_size))\n",
    "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5993902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"hard sigmoid\", useful for binary accuracy calculation from logits\n",
    "def step(values):\n",
    "    # negative values -> 0.0, positive values -> 1.0\n",
    "    return 0.5 * (1.0 + tf.sign(values))\n",
    "\n",
    "\n",
    "# augments images with a probability that is dynamically updated during training\n",
    "class AdaptiveAugmenter(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # stores the current probability of an image being augmented\n",
    "        self.probability = tf.Variable(0.0)\n",
    "\n",
    "        # the corresponding augmentation names from the paper are shown above each layer\n",
    "        # the authors show (see figure 4), that the blitting and geometric augmentations\n",
    "        # are the most helpful in the low-data regime\n",
    "        self.augmenter = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                # blitting/x-flip:\n",
    "                layers.RandomFlip(\"horizontal\"),\n",
    "                # blitting/integer translation:\n",
    "                layers.RandomTranslation(\n",
    "                    height_factor=max_translation,\n",
    "                    width_factor=max_translation,\n",
    "                    interpolation=\"nearest\",\n",
    "                ),\n",
    "                # geometric/rotation:\n",
    "                layers.RandomRotation(factor=max_rotation),\n",
    "                # geometric/isotropic and anisotropic scaling:\n",
    "                layers.RandomZoom(\n",
    "                    height_factor=(-max_zoom, 0.0), width_factor=(-max_zoom, 0.0)\n",
    "                ),\n",
    "            ],\n",
    "            name=\"adaptive_augmenter\",\n",
    "        )\n",
    "\n",
    "    def call(self, images, training):\n",
    "        if training:\n",
    "            augmented_images = self.augmenter(images, training)\n",
    "\n",
    "            # during training either the original or the augmented images are selected\n",
    "            # based on self.probability\n",
    "            augmentation_values = tf.random.uniform(\n",
    "                shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "            )\n",
    "            augmentation_bools = tf.math.less(augmentation_values, self.probability)\n",
    "\n",
    "            images = tf.where(augmentation_bools, augmented_images, images)\n",
    "        return images\n",
    "\n",
    "    def update(self, real_logits):\n",
    "        current_accuracy = tf.reduce_mean(step(real_logits))\n",
    "\n",
    "        # the augmentation probability is updated based on the dicriminator's\n",
    "        # accuracy on real images\n",
    "        accuracy_error = current_accuracy - target_accuracy\n",
    "        self.probability.assign(\n",
    "            tf.clip_by_value(\n",
    "                self.probability + accuracy_error / integration_steps, 0.0, 1.0\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29e1883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN generator\n",
    "def get_generator():\n",
    "    noise_input = keras.Input(shape=(noise_size,))\n",
    "    x = layers.Dense(4 * 4 * width, use_bias=False)(noise_input)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape(target_shape=(4, 4, width))(x)\n",
    "    for _ in range(depth - 1):\n",
    "        x = layers.Conv2DTranspose(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.ReLU()(x)\n",
    "    image_output = layers.Conv2DTranspose(\n",
    "        3, kernel_size=4, strides=2, padding=\"same\", activation=\"sigmoid\",\n",
    "    )(x)\n",
    "\n",
    "    return keras.Model(noise_input, image_output, name=\"generator\")\n",
    "\n",
    "\n",
    "# DCGAN discriminator\n",
    "def get_discriminator():\n",
    "    image_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    x = image_input\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.LeakyReLU(alpha=leaky_relu_slope)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    output_score = layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(image_input, output_score, name=\"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e13bae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_ADA(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.augmenter = AdaptiveAugmenter()\n",
    "        self.generator = get_generator()\n",
    "        self.ema_generator = keras.models.clone_model(self.generator)\n",
    "        self.discriminator = get_discriminator()\n",
    "\n",
    "        self.generator.summary()\n",
    "        self.discriminator.summary()\n",
    "\n",
    "    def compile(self, generator_optimizer, discriminator_optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        # separate optimizers for the two networks\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "\n",
    "        self.generator_loss_tracker = keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.discriminator_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.real_accuracy = keras.metrics.BinaryAccuracy(name=\"real_acc\")\n",
    "        self.generated_accuracy = keras.metrics.BinaryAccuracy(name=\"gen_acc\")\n",
    "        self.augmentation_probability_tracker = keras.metrics.Mean(name=\"aug_p\")\n",
    "        self.kid = KID()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.generator_loss_tracker,\n",
    "            self.discriminator_loss_tracker,\n",
    "            self.real_accuracy,\n",
    "            self.generated_accuracy,\n",
    "            self.augmentation_probability_tracker,\n",
    "            self.kid,\n",
    "        ]\n",
    "\n",
    "    def generate(self, batch_size, training):\n",
    "        latent_samples = tf.random.normal(shape=(batch_size, noise_size))\n",
    "        # use ema_generator during inference\n",
    "        if training:\n",
    "            generated_images = self.generator(latent_samples, training)\n",
    "        else:\n",
    "            generated_images = self.ema_generator(latent_samples, training)\n",
    "        return generated_images\n",
    "\n",
    "    def adversarial_loss(self, real_logits, generated_logits):\n",
    "        # this is usually called the non-saturating GAN loss\n",
    "\n",
    "        real_labels = tf.ones(shape=(batch_size, 1))\n",
    "        generated_labels = tf.zeros(shape=(batch_size, 1))\n",
    "\n",
    "        # the generator tries to produce images that the discriminator considers as real\n",
    "        generator_loss = keras.losses.binary_crossentropy(\n",
    "            real_labels, generated_logits, from_logits=True\n",
    "        )\n",
    "        # the discriminator tries to determine if images are real or generated\n",
    "        discriminator_loss = keras.losses.binary_crossentropy(\n",
    "            tf.concat([real_labels, generated_labels], axis=0),\n",
    "            tf.concat([real_logits, generated_logits], axis=0),\n",
    "            from_logits=True,\n",
    "        )\n",
    "\n",
    "        return tf.reduce_mean(generator_loss), tf.reduce_mean(discriminator_loss)\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        real_images = self.augmenter(real_images, training=True)\n",
    "\n",
    "        # use persistent gradient tape because gradients will be calculated twice\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            generated_images = self.generate(batch_size, training=True)\n",
    "            # gradient is calculated through the image augmentation\n",
    "            generated_images = self.augmenter(generated_images, training=True)\n",
    "\n",
    "            # separate forward passes for the real and generated images, meaning\n",
    "            # that batch normalization is applied separately\n",
    "            real_logits = self.discriminator(real_images, training=True)\n",
    "            generated_logits = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            generator_loss, discriminator_loss = self.adversarial_loss(\n",
    "                real_logits, generated_logits\n",
    "            )\n",
    "\n",
    "        # calculate gradients and update weights\n",
    "        generator_gradients = tape.gradient(\n",
    "            generator_loss, self.generator.trainable_weights\n",
    "        )\n",
    "        discriminator_gradients = tape.gradient(\n",
    "            discriminator_loss, self.discriminator.trainable_weights\n",
    "        )\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, self.generator.trainable_weights)\n",
    "        )\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(discriminator_gradients, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # update the augmentation probability based on the discriminator's performance\n",
    "        self.augmenter.update(real_logits)\n",
    "\n",
    "        self.generator_loss_tracker.update_state(generator_loss)\n",
    "        self.discriminator_loss_tracker.update_state(discriminator_loss)\n",
    "        self.real_accuracy.update_state(1.0, step(real_logits))\n",
    "        self.generated_accuracy.update_state(0.0, step(generated_logits))\n",
    "        self.augmentation_probability_tracker.update_state(self.augmenter.probability)\n",
    "\n",
    "        # track the exponential moving average of the generator's weights to decrease\n",
    "        # variance in the generation quality\n",
    "        for weight, ema_weight in zip(\n",
    "            self.generator.weights, self.ema_generator.weights\n",
    "        ):\n",
    "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "\n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics[:-1]}\n",
    "\n",
    "    def test_step(self, real_images):\n",
    "        generated_images = self.generate(batch_size, training=False)\n",
    "\n",
    "        self.kid.update_state(real_images, generated_images)\n",
    "\n",
    "        # only KID is measured during the evaluation phase for computational efficiency\n",
    "        return {self.kid.name: self.kid.result()}\n",
    "\n",
    "    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6, interval=5):\n",
    "        # plot random generated images for visual evaluation of generation quality\n",
    "        if epoch is None or (epoch + 1) % interval == 0:\n",
    "            num_images = num_rows * num_cols\n",
    "            generated_images = self.generate(num_images, training=False)\n",
    "\n",
    "            plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "            for row in range(num_rows):\n",
    "                for col in range(num_cols):\n",
    "                    index = row * num_cols + col\n",
    "                    plt.subplot(num_rows, num_cols, index + 1)\n",
    "                    plt.imshow(generated_images[index])\n",
    "                    plt.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ed4623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and compile the model\n",
    "model = GAN_ADA()\n",
    "model.compile(\n",
    "    generator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    "    discriminator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    ")\n",
    "\n",
    "# save the best model based on the validation KID metric\n",
    "checkpoint_path = \"gan_model\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    save_weights_only=True,\n",
    "    monitor=\"val_kid\",\n",
    "    mode=\"min\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "# run training and plot generated images periodically\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[\n",
    "        keras.callbacks.LambdaCallback(on_epoch_end=model.plot_images),\n",
    "        checkpoint_callback,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e273f0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model and generate images\n",
    "model.load_weights(checkpoint_path)\n",
    "model.plot_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d09de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
