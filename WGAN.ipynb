{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f07cc0",
   "metadata": {},
   "source": [
    "# WGAN: Wasserstein Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836aca9d",
   "metadata": {},
   "source": [
    "In short, WGAN (the ‘W’ stands for Wasserstein) proposes a **new cost function**.\n",
    "\n",
    "The old version of the GAN minimax optimization game:\n",
    "\n",
    "$$ min_G max_D E_x \\sim p_{data}(x)[\\log D(x)]+ E_z \\sim p_{generated}(z)[1−\\log D(G(z))] $$\n",
    "\n",
    "which approximates a statistical quantity called the Jensen-Shannon divergence.\n",
    "\n",
    "And here is the new one that WGAN uses:\n",
    "\n",
    "$$ min_G max_D E_x \\sim p_{data}(x)[D(x)]+ E_z \\sim p_{generated}(z)[D(G(z))] $$\n",
    "\n",
    "which approximates a statistical quantity called the 1-Wasserstein distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfc43e8",
   "metadata": {},
   "source": [
    "The original **GAN** paper showed that when the discriminator is optimal, the generator is updated in such a way to minimize the Jensen-Shannon divergence.\n",
    "\n",
    "The **Jensen-Shannon divergence** is a way of measuring how different two probability distributions are. The larger the JSD, the more “different” the two distributions are, and vice versa. You compute it like this:\n",
    "\n",
    "$$ JSD(P||Q) = KL(P|| \\frac{P+Q}{2}) + KL(Q|| \\frac{P+Q}{2}) $$\n",
    "\n",
    "$$ KL(A||B)= ∫_{-∞}^{∞}a(x)\\log \\frac{a(x)}{b(x)} dx $$\n",
    "\n",
    "However the authors of the WGAN paper thought that minimizing the JSD is not the best thing to do. Because when the two distributions don’t overlap at all, you can show that the value of the JSD stays at a constant value of \n",
    "$2\\log2$. A function that has has a constant value has a gradient equal to zero, and a zero gradient is bad because it means that the generator learns absolutely nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1fecdb",
   "metadata": {},
   "source": [
    "The alternate distance metric proposed by the **WGAN** authors is the **1-Wasserstein distance**, sometimes called the earth mover distance.\n",
    "\n",
    "It gets the name “earth mover distance” because of an analogy. Imagine that one of the two distributions is a pile of earth, and the other is a pit.\n",
    "\n",
    "The earth mover distance measures the cost of transporting the pile of earth to the pit, assuming that you’re transporting the mud, sand, dirt, etc., as efficiently as possible. Here, “cost” is considered to be distance between point × amount of earth moved.\n",
    "\n",
    "Concretely, the earth mover distance between two distributions can be written as:\n",
    "\n",
    "$$ \n",
    "EMD(P_r, P_{\\theta}) = inf_{\\gamma in \\Pi}, \\sum_{x,y} ||x−y ||\\gamma(x,y) = inf_{\\gamma in \\Pi} E_{(x,y)\\sim \\gamma} ||x-y|| \n",
    "$$\n",
    "\n",
    "Where $inf$ is the infinimum (minimum), $x$ and $y$ are points on the two distributions, and $\\gamma$ is the optimal transport plan.\n",
    "\n",
    "Unfortunately, computing this is intractable. So instead, we compute something totally different:\n",
    "\n",
    "$$\n",
    "EMD(P_r, P_{\\theta}) = sup_{||f||_{L \\leq l}} E_x \\sim P_r f(x) - E_x \\sim P_{\\theta} f(x)\n",
    "$$\n",
    "\n",
    "The connection between these two equations certainly doesn’t seem evident at first, but through some fancy math called the Kantorovich-Rubenstein duality (try saying that three times fast), you can show that these formulas for the Wasserstein/earth mover distance are trying to calculate the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431d5ec7",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc0e8300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(n_epochs=10, batch_size=64, lr=0.0002, b1=0.5, b2=0.999, n_cpu=8, latent_dim=100, img_size=32, channels=1, sample_interval=400, rel_avg_gan=False)\n",
      "[Epoch 0/10] [Batch 0/938] [D loss: 0.694829] [G loss: 0.713984]\n",
      "[Epoch 0/10] [Batch 1/938] [D loss: 0.687632] [G loss: 0.713861]\n",
      "[Epoch 0/10] [Batch 2/938] [D loss: 0.684558] [G loss: 0.713641]\n",
      "[Epoch 0/10] [Batch 3/938] [D loss: 0.677194] [G loss: 0.712993]\n",
      "[Epoch 0/10] [Batch 4/938] [D loss: 0.676178] [G loss: 0.713047]\n",
      "[Epoch 0/10] [Batch 5/938] [D loss: 0.668581] [G loss: 0.712437]\n",
      "[Epoch 0/10] [Batch 6/938] [D loss: 0.665704] [G loss: 0.711283]\n",
      "[Epoch 0/10] [Batch 7/938] [D loss: 0.658166] [G loss: 0.710028]\n",
      "[Epoch 0/10] [Batch 8/938] [D loss: 0.658035] [G loss: 0.709110]\n",
      "[Epoch 0/10] [Batch 9/938] [D loss: 0.652608] [G loss: 0.707713]\n",
      "[Epoch 0/10] [Batch 10/938] [D loss: 0.645431] [G loss: 0.703137]\n",
      "[Epoch 0/10] [Batch 11/938] [D loss: 0.639332] [G loss: 0.700064]\n",
      "[Epoch 0/10] [Batch 12/938] [D loss: 0.632474] [G loss: 0.697416]\n",
      "[Epoch 0/10] [Batch 13/938] [D loss: 0.633752] [G loss: 0.690242]\n",
      "[Epoch 0/10] [Batch 14/938] [D loss: 0.638088] [G loss: 0.679120]\n",
      "[Epoch 0/10] [Batch 15/938] [D loss: 0.633693] [G loss: 0.671381]\n",
      "[Epoch 0/10] [Batch 16/938] [D loss: 0.628284] [G loss: 0.654135]\n",
      "[Epoch 0/10] [Batch 17/938] [D loss: 0.621329] [G loss: 0.641573]\n",
      "[Epoch 0/10] [Batch 18/938] [D loss: 0.629261] [G loss: 0.631008]\n",
      "[Epoch 0/10] [Batch 19/938] [D loss: 0.621801] [G loss: 0.632265]\n",
      "[Epoch 0/10] [Batch 20/938] [D loss: 0.639448] [G loss: 0.604564]\n",
      "[Epoch 0/10] [Batch 21/938] [D loss: 0.654303] [G loss: 0.582905]\n",
      "[Epoch 0/10] [Batch 22/938] [D loss: 0.623150] [G loss: 0.595829]\n",
      "[Epoch 0/10] [Batch 23/938] [D loss: 0.634409] [G loss: 0.588469]\n",
      "[Epoch 0/10] [Batch 24/938] [D loss: 0.638187] [G loss: 0.582696]\n",
      "[Epoch 0/10] [Batch 25/938] [D loss: 0.640462] [G loss: 0.568959]\n",
      "[Epoch 0/10] [Batch 26/938] [D loss: 0.630129] [G loss: 0.570789]\n",
      "[Epoch 0/10] [Batch 27/938] [D loss: 0.635623] [G loss: 0.560853]\n",
      "[Epoch 0/10] [Batch 28/938] [D loss: 0.630537] [G loss: 0.563927]\n",
      "[Epoch 0/10] [Batch 29/938] [D loss: 0.623640] [G loss: 0.567992]\n",
      "[Epoch 0/10] [Batch 30/938] [D loss: 0.609658] [G loss: 0.574998]\n",
      "[Epoch 0/10] [Batch 31/938] [D loss: 0.626610] [G loss: 0.563083]\n",
      "[Epoch 0/10] [Batch 32/938] [D loss: 0.617797] [G loss: 0.568908]\n",
      "[Epoch 0/10] [Batch 33/938] [D loss: 0.610545] [G loss: 0.597052]\n",
      "[Epoch 0/10] [Batch 34/938] [D loss: 0.597772] [G loss: 0.588095]\n",
      "[Epoch 0/10] [Batch 35/938] [D loss: 0.587345] [G loss: 0.600977]\n",
      "[Epoch 0/10] [Batch 36/938] [D loss: 0.618730] [G loss: 0.588348]\n",
      "[Epoch 0/10] [Batch 37/938] [D loss: 0.610780] [G loss: 0.573981]\n",
      "[Epoch 0/10] [Batch 38/938] [D loss: 0.572868] [G loss: 0.602466]\n",
      "[Epoch 0/10] [Batch 39/938] [D loss: 0.579899] [G loss: 0.599844]\n",
      "[Epoch 0/10] [Batch 40/938] [D loss: 0.580077] [G loss: 0.608590]\n",
      "[Epoch 0/10] [Batch 41/938] [D loss: 0.598498] [G loss: 0.586641]\n",
      "[Epoch 0/10] [Batch 42/938] [D loss: 0.567104] [G loss: 0.597633]\n",
      "[Epoch 0/10] [Batch 43/938] [D loss: 0.607702] [G loss: 0.562878]\n",
      "[Epoch 0/10] [Batch 44/938] [D loss: 0.587998] [G loss: 0.553170]\n",
      "[Epoch 0/10] [Batch 45/938] [D loss: 0.604600] [G loss: 0.531118]\n",
      "[Epoch 0/10] [Batch 46/938] [D loss: 0.625393] [G loss: 0.510147]\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/luisagaltarossa/Documents/doc_generative_ai/PyTorch-GAN/implementations/relativistic_gan/relativistic_gan.py\", line 159, in <module>\n",
      "    g_loss.backward()\n",
      "  File \"/Users/luisagaltarossa/venvs/venv_gai/lib/python3.10/site-packages/torch/_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/Users/luisagaltarossa/venvs/venv_gai/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python3 /Users/luisagaltarossa/Documents/doc_generative_ai/PyTorch-GAN/implementations/relativistic_gan/relativistic_gan.py --n_epochs 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90ce084",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
