{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20cb48d",
   "metadata": {},
   "source": [
    "# BigGAN\n",
    "[Paper](https://arxiv.org/abs/1809.11096v2?ref=floydhub-blog): Large Scale GAN Training for High Fidelity Natural Image Synthesis (2018)\n",
    "\n",
    "[Code](https://github.com/huggingface/pytorch-pretrained-BigGAN?ref=floydhub-blog)\n",
    "\n",
    "Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple “truncation trick,” allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator’s input. \n",
    "\n",
    "* We demonstrate that **GANs benefit dramatically from scaling**, and train models with two to four times as many *parameters* and eight times the *batch size* compared to prior art. We introduce two simple, general architectural changes that improve scalability, and modify a regularization scheme to improve conditioning, demonstrably boosting performance.\n",
    "* As a side effect of our modifications, our models become amenable to the **“truncation trick”**, a simple sampling technique that allows explicit, fine-grained control of the trade- off between sample variety and fidelity.\n",
    "\n",
    "Unlike models which need to backpropagate through their latents, GANs can employ an arbitrary prior $p(z)$, yet the vast majority of previous works have chosen to draw $z$ from either $N(0,I)$ or $U[−1, 1]$. They question the optimality of this choice and explore alternatives.\n",
    "\n",
    "Remarkably, our best results come from using a different latent distribution for sampling than was used in training. Taking a model trained with $z \\sim N(0,I)$ and sampling $z$ from a truncated normal (where values which fall outside a range are resampled to fall inside that range) immediately provides a boost to IS and FID. We call this the *Truncation Trick*: truncating a $z$ vector by resampling the values with magnitude above a chosen threshold leads to improvement in individual sample quality at the cost of reduction in overall sample variety."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
