{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7b76d6",
   "metadata": {},
   "source": [
    "# StyleGAN\n",
    "\n",
    "[Paper](https://arxiv.org/abs/1812.04948?ref=floydhub-blog): A Style-Based Generator Architecture for Generative Adversarial Networks\n",
    "\n",
    "[Code](https://github.com/NVlabs/stylegan2-ada-pytorch)\n",
    "\n",
    "They propose an alternative generator architecture for generative adversarial networks, borrowing from *style transfer literature*. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d71c4e22",
   "metadata": {},
   "source": [
    "Motivated by style transfer literature, they re-design the generator architecture in a way that exposes novel ways to control the image synthesis process. Our generator starts from a learned constant input and adjusts the “style” of the image at each convolution layer based on the latent code, therefore directly controlling the strength of image features at different scales.\n",
    "\n",
    "Traditionally the latent code is provided to the generator through an input layer, i.e., the first layer of a feed-forward network (Figure 1a). We depart from this design by omitting the input layer altogether and starting from a learned constant instead (Figure 1b, right). Given a latent code $z$ in the input latent space $Z$, a non-linear mapping network $f : Z \\rightarrow W$ first produces $w \\in W$ (Figure 1b, left). Learned affine transformations then specialize $w$ to styles $y = (y_s, y_b)$ that control adaptive instance normalization (AdaIN) operations after each convolution layer of the synthesis network $g$.\n",
    "\n",
    "Finally, we provide our generator with a direct means to generate stochastic detail by introducing explicit noise inputs. These are single-channel images consisting of un- correlated Gaussian noise, and we feed a dedicated noise image to each layer of the synthesis network. The noise image is broadcasted to all feature maps using learned per- feature scaling factors and then added to the output of the corresponding convolution, as illustrated in Figure 1b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c83ffe",
   "metadata": {},
   "source": [
    "<img src=\"../papers/stylegan.png\" style=\"width:600px;height:450px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01261b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
