{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5746ae52",
   "metadata": {},
   "source": [
    "# GAN: Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2383e1a6",
   "metadata": {},
   "source": [
    "[GAN](https://arxiv.org/abs/1406.2661?ref=floydhub-blog) represents a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8315adc2",
   "metadata": {},
   "source": [
    "Basic GANs consist of two entities, the Generator G and the Discriminator D.\n",
    "\n",
    "**The Generator**\n",
    "* The generator is modeled as a multilayer perceptron\n",
    "* The output of this MLP is an image\n",
    "* The input are variables $\\vec{z}$\n",
    "* The Generator maps the variables $\\vec{z}$ to an image\n",
    "$$\\vec{z} \\rightarrow \\vec{x}$$ \n",
    "where $\\vec{x}$ is the image.\n",
    "* To define a distribution over the image, we define a distribution over the $\\vec{z}$\n",
    "$$ p_z(\\vec{x})$$\n",
    "* The mapping depends on the parameters of the MLP, which we call $\\Theta_g$ and we will denote the whole mapping by \n",
    "$$ G(\\vec{z},\\Theta_g)$$\n",
    "* This defines a probability distribution over images\n",
    "$$ p_g(\\vec{z})$$\n",
    "\n",
    "**The Discriminator**\n",
    "\n",
    "The Discriminator D is a second MLP with separate parameters $\\Theta_d$\n",
    "* The Discriminator takes a single image $\\vec{x}$ as input\n",
    "* It otuputs a single scalar which we denote by $D(\\vec{x},\\Theta_d)$\n",
    "* This scalar is interpreted as the probability that the image came from a given training set of images\n",
    "* $1 - D(\\vec{x},\\Theta_d)$ is the probability that the image came from the generator G\n",
    "\n",
    "**Objective Function**\n",
    "\n",
    "The key to GANs is to understand the objective function.\n",
    "* The Discriminator tries to optimize its parameters $\\Theta_d$ in such a way as to be able to distinguish images from the real dataset and the generator G\n",
    "* The Generator optimizes its parameters $\\Theta_g$ to make images that the Discriminator fails to classify correctly\n",
    "* The whole process can be described as a min − max game:\n",
    "    1. Discriminator: Maximize discriminator prediction accuracy on the combined real and generated images with labels indicating from which source the images came from \n",
    "    2. Generator: Minimize discriminator prediction accuracy on the same dataset\n",
    "* We can define a corresponding value function \n",
    "$$V(D,G)=E_{\\vec{x} \\sim p_{data}(\\vec{x})}[logD(\\vec{x})]+E_{\\vec{z}∼p_{\\vec{z}}} [log(1−D(G(\\vec{z})))] $$\n",
    "\n",
    "The game can be expressed by the contrasting goals of the discriminator and the generator:\n",
    "\n",
    "$$ \\underset{G}{min} \\underset{D}{max} V(D,G)$$\n",
    "\n",
    "In practice, the training is done turn-based\n",
    "* We maximize V with respect to the parameters of the discriminator\n",
    "* Then we minimize V with respect to the parameters of the generator\n",
    "* The maximization is not done in full, since that would lead to very heavy computations\n",
    "* Both can be trained using backpropagation\n",
    "After training, we can use the Generator for making new images!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6856d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/mgalfre/Cgnal/PyTorch-GAN/implementations/gan/gan.py\", line 6, in <module>\r\n",
      "    import torchvision.transforms as transforms\r\n",
      "ModuleNotFoundError: No module named 'torchvision'\r\n"
     ]
    }
   ],
   "source": [
    "! python3 /Users/mgalfre/Cgnal/PyTorch-GAN/implementations/gan/gan.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d2f4232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision (from versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817b8e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
